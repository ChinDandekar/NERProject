{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Name Entity Recognition of Text Corpus Using Transformer Encoder-Based Model\n","\n","This Python notebook does Name Entity Recognition of a Text Corpus, and is broken down as following:\n","\n","### 1. Tokenizer, Data Loader and Dataset\n","\n","### 2. Transformer Encoder-Based Model\n","\n","### 3. Training and Validation Methods\n","\n","### 4. F1 Score Calculation and Test Split Predictions"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:17:58.191961Z","iopub.status.busy":"2023-09-07T06:17:58.191567Z","iopub.status.idle":"2023-09-07T06:18:37.462954Z","shell.execute_reply":"2023-09-07T06:18:37.461575Z","shell.execute_reply.started":"2023-09-07T06:17:58.191925Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.7/site-packages (0.11.1)\n","Requirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.13.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (23.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (4.4.0)\n","Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: pytorch-metric-learning in /opt/conda/lib/python3.7/site-packages (2.3.0)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-metric-learning) (1.13.0)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from pytorch-metric-learning) (1.0.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pytorch-metric-learning) (1.21.6)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pytorch-metric-learning) (4.64.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.4.0)\n","Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pytorch-metric-learning) (1.2.0)\n","Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pytorch-metric-learning) (1.7.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.7/site-packages (1.9.3)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (2023.1.0)\n","Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (0.7.1)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (0.11.1)\n","Requirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (23.0)\n","Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (4.64.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (4.4.0)\n","Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (1.21.6)\n","Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (6.0)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (1.13.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n","Requirement already satisfied: importlib-metadata>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from lightning-utilities>=0.6.0.post0->pytorch-lightning) (4.11.4)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n","Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (0.13.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.0.0->lightning-utilities>=0.6.0.post0->pytorch-lightning) (3.11.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.14)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["from typing import Dict, List, Optional\n","from collections import Counter\n","import os\n","import csv\n","!pip install torchmetrics\n","!pip install pytorch-metric-learning\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","!pip install pytorch-lightning\n","import torch.optim as optim\n","import torchmetrics\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder\n","import numpy"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Tokenizer, Data Loader and Dataset\n","\n","\n","The Tokenizer class contains information about tokenizing a given sentence. The encode method encodes a given text corpus, creating a dictionary of the words that appear in the corpus. The data loader method takes the text corpus and its associated tags and creates a dictionary with the corpus tensor in as 'text' and its associated tags tensor as 'tags'. Finally, the NERDataset creates the final encoded tensor that is ready to be fed into the Transformer model."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:18:37.466386Z","iopub.status.busy":"2023-09-07T06:18:37.465962Z","iopub.status.idle":"2023-09-07T06:18:37.485935Z","shell.execute_reply":"2023-09-07T06:18:37.484659Z","shell.execute_reply.started":"2023-09-07T06:18:37.466341Z"},"id":"u29mNAdI5LSl","trusted":true},"outputs":[],"source":["class Tokenizer:\n","    def __init__(self):\n","        # two special tokens for padding and unknown\n","        self.token2idx = {\"<pad>\": 0, \"<unk>\": 1}\n","        self.idx2token = [\"<pad>\", \"<unk>\"]\n","        self.is_fit = False\n","    \n","    @property\n","    def pad_id(self):\n","        return self.token2idx[\"<pad>\"]\n","    \n","    def __len__(self):\n","        return len(self.idx2token)\n","    \n","    def fit(self, train_texts: List[str]):\n","        counter = Counter()\n","        for text in train_texts:\n","            counter.update(text.lower().strip().split())\n","        \n","        # manually set a vocabulary size for the data set\n","        vocab_size = 20000\n","        self.idx2token.extend([token for token, count in counter.most_common(vocab_size - 2)])\n","        for (i, token) in enumerate(self.idx2token):\n","            self.token2idx[token] = i\n","            \n","        self.is_fit = True\n","                \n","    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n","        if not self.is_fit:\n","            raise Exception(\"Please fit the tokenizer on the training tokens\")\n","            \n","        #first, remove punctuation from text.\n","        splitText = text.lower().strip().split()\n","        \n","        #create the tokens list\n","        limit = -1\n","\n","        tokens = [0]*len(splitText)\n","        if max_length is not None:\n","            if max_length < len(splitText):\n","                limit = max_length\n","                tokens = [0]*max_length\n","        \n","        for i in range(len(tokens)):\n","            if i < len(splitText):\n","                if splitText[i] in self.token2idx:\n","                    curToken = self.token2idx[splitText[i]]\n","                else:\n","                    curToken = self.token2idx[\"<unk>\"]\n","                tokens[i] = curToken\n","            if i == limit:\n","                break\n","        \n","        if max_length is not None:\n","            if max_length > len(splitText):\n","                for x in range(len(splitText), max_length):\n","                    tokens.append(0)\n","        return tokens\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:18:37.488501Z","iopub.status.busy":"2023-09-07T06:18:37.487437Z","iopub.status.idle":"2023-09-07T06:18:37.504937Z","shell.execute_reply":"2023-09-07T06:18:37.503775Z","shell.execute_reply.started":"2023-09-07T06:18:37.488458Z"},"id":"7lHbdxRn5LSm","trusted":true},"outputs":[],"source":["def load_raw_data(filepath: str, with_tags: bool = True):\n","    data = {'text': []}\n","    if with_tags:\n","        data['tags'] = []\n","        with open(filepath) as f:\n","            reader = csv.reader(f)\n","            for text, tags in reader:\n","                data['text'].append(text)\n","                data['tags'].append(tags)\n","    else:\n","        with open(filepath) as f:\n","            for line in f:\n","                data['text'].append(line.strip())\n","    return data"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:18:42.480743Z","iopub.status.busy":"2023-09-07T06:18:42.480281Z","iopub.status.idle":"2023-09-07T06:18:42.882620Z","shell.execute_reply":"2023-09-07T06:18:42.881491Z","shell.execute_reply.started":"2023-09-07T06:18:42.480702Z"},"id":"dEuoJh1Q5LSn","trusted":true},"outputs":[],"source":["data_dir = \"/NERProject/\"\n","tokenizer = Tokenizer()\n","train_raw = load_raw_data(os.path.join(data_dir, \"train.csv\"))\n","val_raw = load_raw_data(os.path.join(data_dir, \"val.csv\"))\n","test_raw = load_raw_data(os.path.join(data_dir, \"test_tokens.txt\"), with_tags=False)\n","# fit the tokenizer on the training tokens\n","tokenizer.fit(train_raw['text'])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:18:55.488730Z","iopub.status.busy":"2023-09-07T06:18:55.488326Z","iopub.status.idle":"2023-09-07T06:18:55.505010Z","shell.execute_reply":"2023-09-07T06:18:55.503159Z","shell.execute_reply.started":"2023-09-07T06:18:55.488694Z"},"id":"KzUsGMealyZb","trusted":true},"outputs":[],"source":["class NERDataset: \n","    tag2idx = {'O': 1, 'B-PER': 2, 'I-PER': 3, 'B-ORG': 4, 'I-ORG': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-MISC': 8, 'I-MISC': 9}\n","    idx2tag = ['<pad>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG','B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n","  \n","    def __init__(self, raw_data: Dict[str, List[str]], tokenizer: Tokenizer, max_length: int = 128):\n","        self.tokenizer = tokenizer\n","        self.token_ids = []\n","        self.tag_ids = []\n","        self.with_tags = False\n","        for text in raw_data['text']:\n","            self.token_ids.append(tokenizer.encode(text, max_length=max_length))\n","        if 'tags' in raw_data:\n","            self.with_tags = True\n","            for tags in raw_data['tags']:\n","                self.tag_ids.append(self.encode_tags(tags, max_length=max_length))\n","    \n","    def encode_tags(self, tags: str, max_length: Optional[int] = None):\n","        tag_ids = [self.tag2idx[tag] for tag in tags.split()]\n","        if max_length is None:\n","            return tag_ids\n","        # truncate the tags if longer than max_length\n","        if len(tag_ids) > max_length:\n","            return tag_ids[:max_length]\n","        # pad with 0s if shorter than max_length\n","        else:\n","            return tag_ids + [0] * (max_length - len(tag_ids))  # 0 as padding for tags\n","        \n","    def __len__(self):\n","        return len(self.token_ids)\n","    \n","    def __getitem__(self, idx):\n","        token_ids = torch.LongTensor(self.token_ids[idx])\n","        \n","        mask = token_ids == self.tokenizer.pad_id  # padding tokens\n","        \n","        if self.with_tags:\n","            # for training and validation\n","            return token_ids, mask, torch.LongTensor(self.tag_ids[idx])\n","        else:\n","            # for testing\n","            return token_ids, mask\n","        "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:18:59.113006Z","iopub.status.busy":"2023-09-07T06:18:59.112597Z","iopub.status.idle":"2023-09-07T06:18:59.722385Z","shell.execute_reply":"2023-09-07T06:18:59.721306Z","shell.execute_reply.started":"2023-09-07T06:18:59.112952Z"},"id":"0kMIKu-p5LSo","trusted":true},"outputs":[],"source":["tr_data = NERDataset(train_raw, tokenizer, max_length = 128)\n","va_data = NERDataset(val_raw, tokenizer, max_length = 128)\n","te_data = NERDataset(test_raw, tokenizer, max_length = 128)"]},{"cell_type":"markdown","metadata":{"id":"QVOHqRsD5LSo"},"source":["## 2. Transformer Encoder-Based Model\n","\n","I used 10 torch.nn.TransformerEncoderLayer layers, a positional encoder and 2 linear layers, along with uniform weight initialization. I found the right hyperparameters by conducting a localized binary search, with the assumption that accuracy as a function of a given hyperparameter is continuous locally."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:26:23.877842Z","iopub.status.busy":"2023-09-07T06:26:23.876945Z","iopub.status.idle":"2023-09-07T06:26:23.899366Z","shell.execute_reply":"2023-09-07T06:26:23.898174Z","shell.execute_reply.started":"2023-09-07T06:26:23.877803Z"},"id":"Jvy2pBQL5LSo","trusted":true},"outputs":[],"source":["class TransformerModel(nn.Module):\n","    \n","    def __init__(self, vocab_size: int, d_model: int, nhead: int, d_hid: int,\n","                 nlayers: int, dropout: float = 0.5, batch_first = False):\n","        super().__init__()\n","        self.model_type = 'Transformer'\n","        \n","        self.encoder = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model, dropout)\n","        \n","        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first = batch_first)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        \n","        self.d_model = d_model\n","        self.linear1 = nn.Linear(d_model, 64)\n","        self.ReLU = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear2 = nn.Linear(64, 10)\n","\n","        self.init_weights()\n","\n","    def init_weights(self) -> None:\n","        initrange = 0.25\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.linear1.bias.data.zero_()\n","        self.linear1.weight.data.uniform_(-initrange, initrange)\n","        self.linear2.bias.data.zero_()\n","        self.linear2.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            src: Tensor, shape [seq_len, batch_size]\n","            src_mask: Tensor, shape [seq_len, seq_len]\n","\n","        Returns:\n","            output Tensor of shape [seq_len, batch_size, ntoken]\n","        \"\"\"\n","        src = self.encoder(src) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src.transpose(1,0), src_key_padding_mask = src_mask)\n","        output = self.linear1(output.transpose(1,0))\n","        output = self.dropout(output)\n","        output = self.ReLU(output)\n","        output = self.linear2(output)\n","        return output\n","    \n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n","        \"\"\"\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Training and Validation Methods\n","\n","Training and validation methods. In the validation method, the inputs are through the model and the resulting logits (output) are used to calculate loss against the validate split labels. In the training method, the same thing happens except we call loss.backwards(), which calculates the derivative of the loss with respect to the parameters in the model at every point in the computational graph (which is implicitly maintained by PyTorch). Then, calling optimizer.step() updates the parameters of the model using Adam (the optimizer we are using for this project)."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:26:27.705346Z","iopub.status.busy":"2023-09-07T06:26:27.704352Z","iopub.status.idle":"2023-09-07T06:26:27.716649Z","shell.execute_reply":"2023-09-07T06:26:27.715337Z","shell.execute_reply.started":"2023-09-07T06:26:27.705290Z"},"id":"Y5Eaibzu5LSp","trusted":true},"outputs":[],"source":["#modify as required\n","def validate(\n","    model: nn.Module, \n","    dataloader: DataLoader, \n","    device: torch.device,\n","):\n","    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n","    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for batch in tqdm(dataloader):\n","            input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n","            # output shape: (batch_size, max_length, num_classes)\n","            logits = model(input_ids, input_mask)\n","            # ignore padding index 0 when calculating loss\n","            loss = F.cross_entropy(logits.reshape(-1,10), tags.reshape(-1), ignore_index = 0)\n","\n","            loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n","            is_active = torch.logical_not(input_mask)  # non-padding elements\n","            # only consider non-padded tokens when calculating accuracy\n","            acc_metric.update(logits[is_active], tags[is_active])\n","\n","    \n","    print(f\"| Validate | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:26:29.456091Z","iopub.status.busy":"2023-09-07T06:26:29.455639Z","iopub.status.idle":"2023-09-07T06:26:29.468535Z","shell.execute_reply":"2023-09-07T06:26:29.467250Z","shell.execute_reply.started":"2023-09-07T06:26:29.456053Z"},"id":"qQtTOXRA5LSp","trusted":true},"outputs":[],"source":["def train(\n","    model: nn.Module, \n","    dataloader: DataLoader, \n","    optimizer: optim.Optimizer,\n","    device: torch.device,\n","    epoch: int,\n","):\n","    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=True).to(device)\n","    loss_metric = torchmetrics.MeanMetric(compute_on_step=True).to(device)\n","    model.train()\n","    # loop through all batches in the training\n","    for batch in tqdm(dataloader):\n","        input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n","        optimizer.zero_grad()\n","\n","        # output shape: (batch_size, max_length, num_classes)\n","        logits = model(input_ids, input_mask)\n","        # ignore padding index 0 when calculating loss\n","        loss = F.cross_entropy(logits.reshape(-1,10), tags.reshape(-1), ignore_index = 0)\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","        loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n","        is_active = torch.logical_not(input_mask)  # non-padding elements\n","        # only consider non-padded tokens when calculating accuracy\n","        acc_metric.update(logits[is_active], tags[is_active])\n","    \n","    print(f\"| Epoch {epoch} | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")\n","    "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:26:30.519240Z","iopub.status.busy":"2023-09-07T06:26:30.518506Z","iopub.status.idle":"2023-09-07T06:26:35.219996Z","shell.execute_reply":"2023-09-07T06:26:35.218856Z","shell.execute_reply.started":"2023-09-07T06:26:30.519202Z"},"id":"5Be4ZCs15LSq","trusted":true},"outputs":[],"source":["torch.manual_seed(42)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# data loaders\n","train_dataloader = DataLoader(tr_data, batch_size=32, shuffle=True, drop_last=False)\n","val_dataloader = DataLoader(va_data, batch_size=32,drop_last = False)\n","test_dataloader = DataLoader(te_data, batch_size=32, drop_last = False)\n","\n","# move the model to device\n","model = TransformerModel(vocab_size = len(tokenizer), \n","    d_model = 128, \n","    d_hid = 128,\n","    nhead = 8, \n","    nlayers = 10, dropout = 0.2, batch_first = False).to(device)\n","\n","\n","optimizer = optim.Adam(model.parameters())\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:37:03.549517Z","iopub.status.busy":"2023-09-07T06:37:03.548723Z","iopub.status.idle":"2023-09-07T06:40:01.860350Z","shell.execute_reply":"2023-09-07T06:40:01.859212Z","shell.execute_reply.started":"2023-09-07T06:37:03.549473Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 439/439 [00:33<00:00, 13.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["| Epoch 0 | loss 0.1674 | acc 0.9505 |\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 102/102 [00:02<00:00, 46.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["| Validate | loss 0.2635 | acc 0.9296 |\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 439/439 [00:33<00:00, 13.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["| Epoch 1 | loss 0.1454 | acc 0.9572 |\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 102/102 [00:02<00:00, 46.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["| Validate | loss 0.2902 | acc 0.9327 |\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 439/439 [00:33<00:00, 13.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["| Epoch 2 | loss 0.1294 | acc 0.9617 |\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 102/102 [00:02<00:00, 46.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["| Validate | loss 0.2640 | acc 0.9339 |\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 439/439 [00:33<00:00, 13.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["| Epoch 3 | loss 0.1171 | acc 0.9652 |\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 102/102 [00:02<00:00, 46.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["| Validate | loss 0.2467 | acc 0.9337 |\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 439/439 [00:33<00:00, 13.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["| Epoch 4 | loss 0.1073 | acc 0.9671 |\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 102/102 [00:02<00:00, 46.96it/s]"]},{"name":"stdout","output_type":"stream","text":["| Validate | loss 0.2952 | acc 0.9350 |\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["for epoch in range(10):\n","    train(model, train_dataloader, optimizer, device, epoch)\n","    validate(model, val_dataloader, device)"]},{"cell_type":"markdown","metadata":{"id":"uQV7JhRl5LSq"},"source":["## 4. F1 Score Calculation and Test Split Predictions\n","\n","Defines the predict method, in which inputs are fed into the model, and the resulting logits are used to predict the NER tags of the input. \n","\n","Runs those tags through conlleval, a script that is used to measure the f1 score of NER. "]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:40:05.162749Z","iopub.status.busy":"2023-09-07T06:40:05.162238Z","iopub.status.idle":"2023-09-07T06:40:05.180382Z","shell.execute_reply":"2023-09-07T06:40:05.179227Z","shell.execute_reply.started":"2023-09-07T06:40:05.162701Z"},"id":"2BeTuu4i5LSq","trusted":true},"outputs":[],"source":["def predict(model: nn.Module, dataloader: DataLoader, device: torch.device) -> List[List[str]]:\n","    model.eval()\n","    preds = []\n","    with torch.no_grad():\n","        for i,batch in enumerate(tqdm(dataloader)):\n","            input_ids, input_mask = batch[0].to(device), batch[1].to(device)\n","            logits = model(input_ids, input_mask)\n","            currPred = numpy.argmax(logits.detach().cpu().numpy(), axis=2)\n","            for index,row in enumerate(currPred):\n","                sentTags = []\n","                for rowIndex, value in enumerate(row):\n","                    if input_mask[index][rowIndex]:\n","                        break\n","                    else:\n","                        sentTags.append(NERDataset.idx2tag[value])\n","                preds.append(sentTags)\n","    return preds"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:40:05.990607Z","iopub.status.busy":"2023-09-07T06:40:05.989538Z","iopub.status.idle":"2023-09-07T06:40:07.051906Z","shell.execute_reply":"2023-09-07T06:40:07.050601Z","shell.execute_reply.started":"2023-09-07T06:40:05.990556Z"},"id":"EzFjEe0c5LSq","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-09-07 06:40:06--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7502 (7.3K) [text/plain]\n","Saving to: ‘conlleval.py.2’\n","\n","conlleval.py.2      100%[===================>]   7.33K  --.-KB/s    in 0s      \n","\n","2023-09-07 06:40:06 (50.7 MB/s) - ‘conlleval.py.2’ saved [7502/7502]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n","from conlleval import evaluate"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:40:07.055508Z","iopub.status.busy":"2023-09-07T06:40:07.054767Z","iopub.status.idle":"2023-09-07T06:40:10.251617Z","shell.execute_reply":"2023-09-07T06:40:10.250454Z","shell.execute_reply.started":"2023-09-07T06:40:07.055461Z"},"id":"lVAnQYdD5LSr","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 102/102 [00:03<00:00, 33.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["processed 54612 tokens with 5942 phrases; found: 5472 phrases; correct: 4011.\n","accuracy:  64.85%; (non-O)\n","accuracy:  93.88%; precision:  73.30%; recall:  67.50%; FB1:  70.28\n","              LOC: precision:  85.03%; recall:  79.80%; FB1:  82.34  1724\n","             MISC: precision:  76.44%; recall:  73.21%; FB1:  74.79  883\n","              ORG: precision:  63.97%; recall:  60.10%; FB1:  61.98  1260\n","              PER: precision:  66.29%; recall:  57.76%; FB1:  61.73  1605\n"]},{"data":{"text/plain":["(73.30043859649122, 67.50252440255807, 70.28210968985456)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# use the conlleval script to measure the entity-level f1\n","pred_tags = []\n","tag_count = 0\n","append_O_count = 0\n","len_idx = []\n","for idx, tags in enumerate(predict(model, val_dataloader, device)):\n","    tag_count += len(tags)\n","    len_idx.append(len(tags))\n","    pred_tags.extend(tags)\n","    append_O_count += 1\n","    pred_tags.append('O')\n","        \n","true_tags = []\n","true_tag_count = 0\n","append_O_count = 0\n","\n","for idx,tags in enumerate(val_raw['tags']):\n","    true_tag_count += len(tags.strip().split())\n","    true_tags.extend(tags.strip().split())\n","    append_O_count += 1\n","    true_tags.append('O')\n","    \n","evaluate(true_tags, pred_tags, verbose=True)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:40:48.160322Z","iopub.status.busy":"2023-09-07T06:40:48.159550Z","iopub.status.idle":"2023-09-07T06:40:51.163373Z","shell.execute_reply":"2023-09-07T06:40:51.162134Z","shell.execute_reply.started":"2023-09-07T06:40:48.160282Z"},"id":"dVt102qy5LSs","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 108/108 [00:02<00:00, 36.16it/s]\n"]}],"source":["preds = predict(model, test_dataloader, device)\n","with open(\"submission.txt\", \"w\") as f:\n","    for tags in preds:\n","        f.write(\" \".join(tags) + \"\\n\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
